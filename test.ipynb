# Multi-Label Binary Classification Baseline Pipeline
# This notebook provides a comprehensive baseline for multi-label classification
# with mixed numerical and categorical features

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, multilabel_confusion_matrix, hamming_loss, jaccard_score
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from category_encoders import TargetEncoder, BinaryEncoder, HashingEncoder
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

print("Multi-Label Binary Classification Baseline Pipeline")
print("=" * 50)

## 1. Data Generation (Replace this section with your actual data loading)

def generate_sample_data(n_samples=5000):
    """
    Generate sample multi-label dataset with mixed feature types
    Replace this function with your actual data loading code
    """
    np.random.seed(42)
    
    # Numerical features
    num_features = pd.DataFrame({
        'num_feat_1': np.random.normal(0, 1, n_samples),
        'num_feat_2': np.random.exponential(2, n_samples),
        'num_feat_3': np.random.uniform(-5, 5, n_samples),
        'num_feat_4': np.random.gamma(2, 2, n_samples)
    })
    
    # Low cardinality categorical features
    low_cat_features = pd.DataFrame({
        'cat_low_1': np.random.choice(['A', 'B', 'C', 'D'], n_samples, p=[0.4, 0.3, 0.2, 0.1]),
        'cat_low_2': np.random.choice(['X', 'Y', 'Z'], n_samples, p=[0.5, 0.3, 0.2]),
        'cat_low_3': np.random.choice(['Type1', 'Type2', 'Type3', 'Type4', 'Type5'], n_samples)
    })
    
    # High cardinality categorical features
    high_cat_features = pd.DataFrame({
        'cat_high_1': [f'cat1_{i}' for i in np.random.randint(0, 1000, n_samples)],
        'cat_high_2': [f'cat2_{i}' for i in np.random.randint(0, 800, n_samples)],
        'cat_high_3': [f'cat3_{i}' for i in np.random.randint(0, 1200, n_samples)]
    })
    
    # Combine all features
    X = pd.concat([num_features, low_cat_features, high_cat_features], axis=1)
    
    # Generate correlated multi-label targets
    # Create some correlation between features and labels
    prob_a = 1 / (1 + np.exp(-(X['num_feat_1'] + (X['cat_low_1'] == 'A').astype(int))))
    prob_b = 1 / (1 + np.exp(-(X['num_feat_2'] - 2 + (X['cat_low_2'] == 'X').astype(int))))
    prob_c = 1 / (1 + np.exp(-(X['num_feat_3'] + X['num_feat_4'] * 0.5)))
    
    y = pd.DataFrame({
        'is_a': np.random.binomial(1, prob_a),
        'is_b': np.random.binomial(1, prob_b),
        'is_c': np.random.binomial(1, prob_c)
    })
    
    return X, y

# Generate sample data (replace with your data loading)
print("Loading data...")
X, y = generate_sample_data(5000)
print(f"Data shape: X={X.shape}, y={y.shape}")

## 2. Exploratory Data Analysis

print("\n" + "="*30)
print("EXPLORATORY DATA ANALYSIS")
print("="*30)

# Basic info about features
print("\nFeature Info:")
print(X.info())
print("\nTarget Distribution:")
print(y.describe())

# Identify feature types
numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=[object]).columns.tolist()

print(f"\nNumerical features ({len(numerical_features)}): {numerical_features}")
print(f"Categorical features ({len(categorical_features)}): {categorical_features}")

# Analyze categorical feature cardinality
print("\nCategorical Feature Cardinality:")
cat_cardinality = {}
for col in categorical_features:
    cardinality = X[col].nunique()
    cat_cardinality[col] = cardinality
    print(f"{col}: {cardinality} unique values")

# Separate high and low cardinality categorical features
high_cardinality_threshold = 50
low_cat_features = [col for col, card in cat_cardinality.items() if card <= high_cardinality_threshold]
high_cat_features = [col for col, card in cat_cardinality.items() if card > high_cardinality_threshold]

print(f"\nLow cardinality categorical ({len(low_cat_features)}): {low_cat_features}")
print(f"High cardinality categorical ({len(high_cat_features)}): {high_cat_features}")

# Label correlation analysis
print("\nLabel Correlations:")
label_corr = y.corr()
print(label_corr)

# Visualize label distributions and correlations
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Label distributions
y.sum().plot(kind='bar', ax=axes[0], title='Label Frequencies')
axes[0].set_ylabel('Count')
axes[0].tick_params(axis='x', rotation=45)

# Label correlations heatmap
sns.heatmap(label_corr, annot=True, cmap='coolwarm', center=0, ax=axes[1])
axes[1].set_title('Label Correlations')

plt.tight_layout()
plt.show()

## 3. Preprocessing Pipeline Setup

print("\n" + "="*30)
print("PREPROCESSING PIPELINE")
print("="*30)

class MultiLabelPreprocessor:
    """
    Custom preprocessor for multi-label classification with mixed feature types
    """
    def __init__(self, 
                 numerical_features, 
                 low_cat_features, 
                 high_cat_features,
                 high_cat_method='hashing',
                 hash_n_components=100):
        
        self.numerical_features = numerical_features
        self.low_cat_features = low_cat_features
        self.high_cat_features = high_cat_features
        self.high_cat_method = high_cat_method
        self.hash_n_components = hash_n_components
        
        # Define transformers for different feature types
        transformers = []
        
        # Numerical features: StandardScaler
        if numerical_features:
            transformers.append(('num', StandardScaler(), numerical_features))
        
        # Low cardinality categorical: One-hot encoding
        if low_cat_features:
            from sklearn.preprocessing import OneHotEncoder
            transformers.append(('cat_low', OneHotEncoder(drop='first', handle_unknown='ignore'), low_cat_features))
        
        # High cardinality categorical: Multiple options
        if high_cat_features:
            if high_cat_method == 'hashing':
                transformers.append(('cat_high', HashingEncoder(n_components=hash_n_components), high_cat_features))
            elif high_cat_method == 'binary':
                transformers.append(('cat_high', BinaryEncoder(), high_cat_features))
            elif high_cat_method == 'target':
                transformers.append(('cat_high', TargetEncoder(), high_cat_features))
        
        self.preprocessor = ColumnTransformer(
            transformers=transformers,
            remainder='passthrough'
        )
    
    def fit_transform(self, X, y=None):
        return self.preprocessor.fit_transform(X, y)
    
    def transform(self, X):
        return self.preprocessor.transform(X)
    
    def fit(self, X, y=None):
        self.preprocessor.fit(X, y)
        return self

# Create different preprocessing strategies for comparison
preprocessing_strategies = {
    'hashing': MultiLabelPreprocessor(
        numerical_features, low_cat_features, high_cat_features, 
        high_cat_method='hashing', hash_n_components=100
    ),
    'binary': MultiLabelPreprocessor(
        numerical_features, low_cat_features, high_cat_features, 
        high_cat_method='binary'
    )
}

print("Available preprocessing strategies:")
for strategy in preprocessing_strategies.keys():
    print(f"- {strategy}")

## 4. Model Pipeline Setup

print("\n" + "="*30)
print("MODEL PIPELINE SETUP")
print("="*30)

def create_model_pipelines(preprocessor):
    """
    Create different model pipelines for comparison
    """
    models = {
        'Logistic Regression': Pipeline([
            ('preprocessor', preprocessor),
            ('classifier', MultiOutputClassifier(
                LogisticRegression(random_state=42, max_iter=1000)
            ))
        ]),
        
        'Random Forest': Pipeline([
            ('preprocessor', preprocessor),
            ('classifier', MultiOutputClassifier(
                RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
            ))
        ])
    }
    
    return models

## 5. Evaluation Metrics

def evaluate_multilabel_model(y_true, y_pred, model_name="Model"):
    """
    Comprehensive evaluation for multi-label classification
    """
    print(f"\n{model_name} Evaluation:")
    print("-" * 40)
    
    # Overall metrics
    hamming = hamming_loss(y_true, y_pred)
    jaccard = jaccard_score(y_true, y_pred, average='samples')
    
    # Per-label metrics
    f1_micro = f1_score(y_true, y_pred, average='micro')
    f1_macro = f1_score(y_true, y_pred, average='macro')
    f1_samples = f1_score(y_true, y_pred, average='samples')
    
    precision_micro = precision_score(y_true, y_pred, average='micro')
    precision_macro = precision_score(y_true, y_pred, average='macro')
    
    recall_micro = recall_score(y_true, y_pred, average='micro')
    recall_macro = recall_score(y_true, y_pred, average='macro')
    
    print(f"Hamming Loss: {hamming:.4f}")
    print(f"Jaccard Score (samples): {jaccard:.4f}")
    print(f"F1 Score (micro): {f1_micro:.4f}")
    print(f"F1 Score (macro): {f1_macro:.4f}")
    print(f"F1 Score (samples): {f1_samples:.4f}")
    print(f"Precision (micro): {precision_micro:.4f}")
    print(f"Precision (macro): {precision_macro:.4f}")
    print(f"Recall (micro): {recall_micro:.4f}")
    print(f"Recall (macro): {recall_macro:.4f}")
    
    # Per-label detailed metrics
    print(f"\nPer-Label Classification Report:")
    print(classification_report(y_true, y_pred, target_names=y_true.columns))
    
    return {
        'hamming_loss': hamming,
        'jaccard_score': jaccard,
        'f1_micro': f1_micro,
        'f1_macro': f1_macro,
        'f1_samples': f1_samples,
        'precision_micro': precision_micro,
        'precision_macro': precision_macro,
        'recall_micro': recall_micro,
        'recall_macro': recall_macro
    }

## 6. Train-Test Split

print("\n" + "="*30)
print("DATA SPLITTING")
print("="*30)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y['is_a']  # Stratify on one label
)

print(f"Training set: X={X_train.shape}, y={y_train.shape}")
print(f"Test set: X={X_test.shape}, y={y_test.shape}")

## 7. Model Training and Evaluation

print("\n" + "="*30)
print("MODEL TRAINING AND EVALUATION")
print("="*30)

results = {}

# Test different preprocessing strategies
for prep_name, preprocessor in preprocessing_strategies.items():
    print(f"\n{'='*20}")
    print(f"PREPROCESSING STRATEGY: {prep_name.upper()}")
    print(f"{'='*20}")
    
    # Create model pipelines
    models = create_model_pipelines(preprocessor)
    
    # Train and evaluate each model
    for model_name, pipeline in models.items():
        print(f"\nTraining {model_name} with {prep_name} preprocessing...")
        
        # Fit the model
        pipeline.fit(X_train, y_train)
        
        # Make predictions
        y_pred = pipeline.predict(X_test)
        
        # Evaluate
        metrics = evaluate_multilabel_model(y_test, y_pred, f"{model_name} ({prep_name})")
        
        # Store results
        results[f"{model_name}_{prep_name}"] = metrics

## 8. Results Comparison

print("\n" + "="*30)
print("RESULTS COMPARISON")
print("="*30)

# Create results DataFrame
results_df = pd.DataFrame(results).T
print("\nModel Comparison Summary:")
print(results_df.round(4))

# Find best model for each metric
print("\nBest Models by Metric:")
for metric in results_df.columns:
    if metric == 'hamming_loss':  # Lower is better for hamming loss
        best_model = results_df[metric].idxmin()
        best_score = results_df[metric].min()
    else:  # Higher is better for other metrics
        best_model = results_df[metric].idxmax()
        best_score = results_df[metric].max()
    print(f"{metric}: {best_model} ({best_score:.4f})")

# Visualize results
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.ravel()

metrics_to_plot = ['f1_macro', 'f1_micro', 'jaccard_score', 'hamming_loss']
for i, metric in enumerate(metrics_to_plot):
    results_df[metric].plot(kind='bar', ax=axes[i], title=f'{metric.replace("_", " ").title()}')
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

## 9. Feature Importance Analysis (for tree-based models)

print("\n" + "="*30)
print("FEATURE IMPORTANCE ANALYSIS")
print("="*30)

# Get feature importance from Random Forest models
for prep_name in preprocessing_strategies.keys():
    model_key = f"Random Forest_{prep_name}"
    if model_key in [f"Random Forest_{prep_name}" for prep_name in preprocessing_strategies.keys()]:
        print(f"\nFeature Importance - Random Forest ({prep_name}):")
        
        # Re-train the model to access feature importance
        pipeline = create_model_pipelines(preprocessing_strategies[prep_name])['Random Forest']
        pipeline.fit(X_train, y_train)
        
        # Get feature names after preprocessing
        preprocessor = pipeline.named_steps['preprocessor']
        
        # For multi-output classifier, we need to access individual estimators
        classifier = pipeline.named_steps['classifier']
        
        # Average feature importance across all labels
        n_features = preprocessor.transform(X_train[:5]).shape[1]
        avg_importance = np.zeros(n_features)
        
        for estimator in classifier.estimators_:
            avg_importance += estimator.feature_importances_
        
        avg_importance /= len(classifier.estimators_)
        
        # Get top 20 most important features
        top_indices = np.argsort(avg_importance)[-20:][::-1]
        top_importance = avg_importance[top_indices]
        
        # Plot feature importance
        plt.figure(figsize=(10, 6))
        plt.barh(range(len(top_importance)), top_importance)
        plt.yticks(range(len(top_importance)), [f'Feature_{i}' for i in top_indices])
        plt.xlabel('Average Feature Importance')
        plt.title(f'Top 20 Feature Importance - Random Forest ({prep_name})')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

## 10. Recommendations and Next Steps

print("\n" + "="*30)
print("RECOMMENDATIONS AND NEXT STEPS")
print("="*30)

print("""
BASELINE MODEL RECOMMENDATIONS:
1. Based on the results above, choose the best performing model/preprocessing combination
2. Consider the trade-off between performance and interpretability
3. For high-cardinality categorical features, hashing encoder often provides good balance

NEXT STEPS FOR IMPROVEMENT:
1. Hyperparameter Tuning:
   - Use GridSearchCV or RandomizedSearchCV
   - Tune preprocessing parameters (e.g., hash dimensions)
   - Optimize model-specific parameters

2. Advanced Models:
   - XGBoost/LightGBM with MultiOutputClassifier
   - Neural networks with appropriate architecture
   - Ensemble methods combining different approaches

3. Feature Engineering:
   - Create interaction features
   - Apply domain-specific transformations
   - Feature selection techniques

4. Advanced Preprocessing:
   - Target encoding with cross-validation
   - Embeddings for categorical features
   - Feature scaling alternatives (RobustScaler, etc.)

5. Class Imbalance Handling:
   - SMOTE for multi-label scenarios
   - Class weight adjustments
   - Threshold optimization

6. Cross-Validation:
   - Implement proper multi-label stratified CV
   - Nested CV for unbiased performance estimation

MONITORING IN PRODUCTION:
- Track per-label performance metrics
- Monitor feature drift for high-cardinality categories
- Set up alerting for performance degradation
""")

print("Baseline pipeline complete! Use this as starting point for your multi-label classification project.")
